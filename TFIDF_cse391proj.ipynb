{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasize:  27877\n",
      "Drop empty content...\n",
      "new datasize:  27871\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#parameters\n",
    "directory =\"/Users/weichen/Downloads/\"\n",
    "file_name = \"title_content_2gram_processed.csv\"\n",
    "to_file_name = directory+ \"vector_title_content.csv\"#####\n",
    "text_column_names = [\"title_new\",\"content_new\"]\n",
    "label_column_name = \"label\"\n",
    "one_hot_column_name = \"publication\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "def drop_nan_epmtystr(df,subset=['content_new']):\n",
    "    df.replace(r'^\\s*$', np.nan, regex=True,inplace=True)\n",
    "    df.replace('', np.nan, inplace=True)\n",
    "    print(\"datasize: \",len(df))\n",
    "    df.dropna(subset=subset, inplace=True)\n",
    "    print(\"Drop empty content...\")\n",
    "    print(\"new datasize: \",len(df))\n",
    "    return df\n",
    "\n",
    "#read data\n",
    "df = pd.read_csv(directory+file_name)\n",
    "#always drop so code doesnt crash later\n",
    "df = drop_nan_epmtystr(df,text_column_names)\n",
    "df.head()\n",
    "\n",
    "max_feature = [441,32]\n",
    "# min_df = [.1,.001]\n",
    "vectorizers = []\n",
    "for index, column_name in enumerate(text_column_names):\n",
    "    df2 = df.copy(deep=True)\n",
    "    vectorizer = TfidfVectorizer(stop_words = 'english',decode_error='ignore',max_features=max_feature[index])\n",
    "    vectorizer.fit(df2[column_name])\n",
    "    vectorizers.append(vectorizer)\n",
    "\n",
    "def text2features(df,text_column_names,vectorizers):\n",
    "    df_vec_list = []\n",
    "    for index, text_column_name in enumerate(text_column_names):\n",
    "        vectorizer = vectorizers[index]\n",
    "        print(index)\n",
    "        X = vectorizer.transform(df[column_name])\n",
    "\n",
    "        df_vec = pd.DataFrame(df['label'][:df.shape[0]])\n",
    "        featureNameList = vectorizer.get_feature_names()\n",
    "        for i in range(X.shape[1]):\n",
    "            df_vec[featureNameList[i]] = X[:,i].toarray()\n",
    "        print(df_vec) \n",
    "        df_vec_list.append(df_vec)\n",
    "    df_concat = pd.concat([df_vec_list[0],df_vec_list[1].iloc[:,1:]],axis=1)\n",
    "    return df_concat\n",
    "\n",
    "# df_concat = text2features(df,text_column_names,vectorizers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 41.3 s, sys: 1.76 s, total: 43 s\n",
      "Wall time: 45.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#parameters\n",
    "directory =\"/Users/weichen/Downloads/\"\n",
    "file_name = \"title_content_2gram_processed.csv\"\n",
    "text_column_names = [\"title_new\",\"content_new\"]\n",
    "label_column_name = \"label\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def drop_nan_epmtystr(df,subset=['content_new']):\n",
    "    df.replace(r'^\\s*$', np.nan, regex=True,inplace=True)\n",
    "    df.replace('', np.nan, inplace=True)\n",
    "    df.dropna(subset=subset, inplace=True)\n",
    "    return df\n",
    "\n",
    "#read data\n",
    "df = pd.read_csv(directory+file_name)\n",
    "#always drop so code doesnt crash later\n",
    "df = drop_nan_epmtystr(df,text_column_names)\n",
    "df.head()\n",
    "\n",
    "max_feature = [441,32]\n",
    "# min_df = [.1,.001]\n",
    "vectorizers = []\n",
    "for index, column_name in enumerate(text_column_names):\n",
    "    df2 = df.copy(deep=True)\n",
    "    vectorizer = TfidfVectorizer(stop_words = 'english',decode_error='ignore',max_features=max_feature[index])\n",
    "    vectorizer.fit(df2[column_name])\n",
    "    vectorizers.append(vectorizer)\n",
    "\n",
    "def text2features(df,text_column_names,vectorizers):\n",
    "    df_vec_list = []\n",
    "    for index, text_column_name in enumerate(text_column_names):\n",
    "        vectorizer = vectorizers[index]\n",
    "#         print(index)\n",
    "        X = vectorizer.transform(df[column_name])\n",
    "\n",
    "        df_vec = pd.DataFrame(df['label'][:df.shape[0]])\n",
    "        featureNameList = vectorizer.get_feature_names()\n",
    "        for i in range(X.shape[1]):\n",
    "            df_vec[featureNameList[i]] = X[:,i].toarray()\n",
    "#         print(df_vec) \n",
    "        df_vec_list.append(df_vec)\n",
    "    df_concat = pd.concat([df_vec_list[0],df_vec_list[1].iloc[:,1:]],axis=1)\n",
    "    return df_concat\n",
    "\n",
    "df_concat = text2features(df,text_column_names,vectorizers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27871, 474)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>access</th>\n",
       "      <th>accuse</th>\n",
       "      <th>act</th>\n",
       "      <th>administration</th>\n",
       "      <th>admit</th>\n",
       "      <th>age</th>\n",
       "      <th>ahead</th>\n",
       "      <th>air</th>\n",
       "      <th>al</th>\n",
       "      <th>...</th>\n",
       "      <th>think</th>\n",
       "      <th>time</th>\n",
       "      <th>trump</th>\n",
       "      <th>vote</th>\n",
       "      <th>want</th>\n",
       "      <th>way</th>\n",
       "      <th>woman</th>\n",
       "      <th>work</th>\n",
       "      <th>world</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fake</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.325802</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.261244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fake</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.237258</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.460895</td>\n",
       "      <td>0.344942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fake</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.144624</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fake</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.565723</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.590405</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fake</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.190163</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.208835</td>\n",
       "      <td>0.323498</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.30898</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 474 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  label  access  accuse       act  administration  admit  age  ahead  \\\n",
       "0  fake     0.0     0.0  0.000000        0.000000    0.0  0.0    0.0   \n",
       "1  fake     0.0     0.0  0.000000        0.237258    0.0  0.0    0.0   \n",
       "2  fake     0.0     0.0  0.000000        0.000000    0.0  0.0    0.0   \n",
       "3  fake     0.0     0.0  0.000000        0.000000    0.0  0.0    0.0   \n",
       "4  fake     0.0     0.0  0.190163        0.000000    0.0  0.0    0.0   \n",
       "\n",
       "        air   al    ...     think  time     trump      vote      want  \\\n",
       "0  0.000000  0.0    ...       0.0   0.0  0.000000  0.000000  0.000000   \n",
       "1  0.000000  0.0    ...       0.0   0.0  0.000000  0.000000  0.000000   \n",
       "2  0.144624  0.0    ...       0.0   0.0  0.000000  0.000000  0.000000   \n",
       "3  0.000000  0.0    ...       0.0   0.0  0.565723  0.000000  0.590405   \n",
       "4  0.000000  0.0    ...       0.0   0.0  0.000000  0.208835  0.323498   \n",
       "\n",
       "        way  woman     work     world      year  \n",
       "0  0.325802    0.0  0.00000  0.000000  0.261244  \n",
       "1  0.000000    0.0  0.00000  0.460895  0.344942  \n",
       "2  0.000000    0.0  0.00000  0.000000  0.000000  \n",
       "3  0.000000    0.0  0.00000  0.000000  0.000000  \n",
       "4  0.000000    0.0  0.30898  0.000000  0.000000  \n",
       "\n",
       "[5 rows x 474 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df_concat.shape)\n",
    "df_concat.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing to:  /Users/weichen/Downloads/vector_title_content.csv\n"
     ]
    }
   ],
   "source": [
    "print(\"writing to: \",to_file_name)\n",
    "df_concat.to_csv(to_file_name, encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27871, 250)\n"
     ]
    }
   ],
   "source": [
    "df_hot = pd.get_dummies(df[one_hot_column_name])\n",
    "print(df_hot.shape)\n",
    "df_concat_hot = pd.concat([df_concat,df_hot],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas.get_dummies(data, prefix=None, prefix_sep='_', dummy_na=False, columns=None, sparse=False, drop_first=False, dtype=None)[source]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_file_name = directory+ \"vector_title_content_tfidf_publication_1hot.csv\"#####\n",
    "print(\"writing to: \",to_file_name)\n",
    "df_concat_hot.to_csv(to_file_name, encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, text_column_name in enumerate(text_column_names):\n",
    "    print(df2[text_column_name].head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
